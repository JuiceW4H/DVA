## EFS Architecture

* Provides network-based file systems which can be mounted within Linux EC2 Instances (/nfs/media) and used by multiple instances at once.
* An AWS implementation of a shared storage standard called Network File System (NFSv4).
* Exists separately and independently of an EC2 Instance Lifecycle.
* Private service by Default which means it’s isolated to the VPC it’s provisioned.
* Mounted via mount targets.
* Can be accessed from on-premises via VPC Peering, VPN Connections, or AWS Direct Connect.

**EFS Architecture**

![Elastic File System (EFS) - Network Storage and Data Lifecycle-07-28-2024](images/Elastic%20File%20System%20(EFS)%20-%20Network%20Storage%20and%20Data%20Lifecycle-07-28-2024.png)

Mount Targets
	\- provide network access points to an EFS file system from within a Virtual Private Cloud (VPC).
	\- takes an IP address from the subnet they’re inside in.

POSIX Permissions
	\- permission definition of Users, Groups, and Others inside Linux.

**Key Concepts**

* Linux Only

**2 Types Performance Modes**

* General Purpose (Default)
	\- for latency sensitive use cases.

* Max IO
	\- scales to higher levels of aggregate throughput and operations per seconds.
	\- tradeoff of increases latencies.
	\- highly parallel workload.

**2 Types of Throughput Modes**

* Bursting
	\- similar to GP volumes in EBS volumes as it has a burst pool.
	\- throughput scales based on the size of the file system.

* Provisioned
	\- you can specify throughput requirements separately from size.
	\- similar to IO volumes.

**3 Types of Storage Classes**

* You can use Lifecycle policies to move data between classes.

1. Standard (Default)
	\- used for frequently accessed files.

2. Infrequent Access (IA)
	\- lower cost storage class.
	\- designed for infrequently access data.

3. Archive

**A4L Current Architecture**

![Elastic File System (EFS) - Network Storage and Data Lifecycle-07-28-2024-1](images/Elastic%20File%20System%20(EFS)%20-%20Network%20Storage%20and%20Data%20Lifecycle-07-28-2024-1.png)

## FSx for Windows File Server

* Provides fully managed native Windows File Servers/File Shares.
* Designed for integration with Windows environments.
* It can integrate with either a managed Active Directory, or Self-Managed AD running inside AWS or on-premises.
* Can be deployed in Single or Multi-AZ mode within a VPC.
* Can perform On-Demand and Scheduled backups.
* Accessible using VPC, Peering, VPN, Direct Connect.

**FSx Implementation Architecture**

![Hybrid Environments and Migration-08-04-2024-27](images/Hybrid%20Environments%20and%20Migration-08-04-2024-27.png)

* Native windows file system, supports de-duplication (sub file), Distributed File System (DFS), KMS at-rest encryption and enforced encryption in-transit.
* Supports volume shadow copies (File Level Versioning).
* Can deliver anywhere from 8MB/s to 2GB/s.
* Can deliver 100,000’s of IOPS.
* Less than 1 ms of latency.

**Key Features and Benefits**

* Provides access to Native File Systems over SMB.
* Makes use of the Windows Permission Model.
* Can be integrated with Directory Service or your own Directory.

Volume Shadow Copy Service (VSS)
	\- a windows feature that allows users to perform file and folder level restores.

Distributed File System (DFS)
	\- a way that you can natively scale out file systems inside Windows Environments.
	\- grouping of file shares together in 1 enterprise wide structure or replication or scaling out performance.

## FSx for Lustre

* File System designed for high performance computing workloads.
* A managed implementation of the Lustre file system.
* Supports Linux based instances and POSIX style permissions for file systems.
* Used in Big Data, ML or Financial Modelling.
* Can scale to 100GB/s of throughput and sub millisecond latency.
* Accessible over VPN or Direct Connect.

**2 Different Deployment Types for Lustre**

* Minimum Size is 1.2TiB with increments of 2.4TiB.

1. Scratch
	\- absolute best performance for short term/temporary workloads.
	\- no HA and Replication.
	\- speed of 200MB/s per TiB of storage.

2. Persistent
	\- great for long term storage.
	\- High Availability in 1 AZ only.
	\- has self-healing.
	\- offers 50MB/s, 100MB/s and 200MB/s per TiB of storage.

**Conceptual**

![Hybrid Environments and Migration-08-04-2024-28](images/Hybrid%20Environments%20and%20Migration-08-04-2024-28.png)

* File system is where the data lives.
* You can associate it with a repository (S3).
* Objects in the associated repository will be visible in the File system.
* Data is Lazy Loaded from S3 into the File System as needed.

hsm_archive
	\- command to sync any changes made in the File System to the S3 Bucket. 

**Key Points**

* Metadata is stored in Metadata Targets (MST).
* Objects are stored on called object storage targets (OSTs) (1.17TiB each).
* Baseline performance is based on size.
* Minimum Size is 1.2TiB with increments of 2.4TiB.
* For Scratch the base performance is 200MB/s per TiB of storage.
* Persistent offers 50MB/s, 100MB/s and 200MB/s per TiB of storage.
* For both types you can burst up to 1,300MB/s per TiB (Credit System).

**Lustre Architecture**

![Hybrid Environments and Migration-08-04-2024-29](images/Hybrid%20Environments%20and%20Migration-08-04-2024-29.png)

**More Key Points**

![Hybrid Environments and Migration-08-04-2024-30](images/Hybrid%20Environments%20and%20Migration-08-04-2024-30.png)
